# Introduction

These are our notes for the LLM Paper Club (Asia Edition!) run out of the `latent-space` discord channel. If you're interested in attending, you can check out the event calender [here](https://lu.ma/ls) of all the latent space event. 

## Simple Rules

In short, each week we discuss a LLM paper. We'd appreciate if you could read the paper beforehand so that when we work through it, it'd be a fruitful discussion for everyone. I'd like to emphasize here that **this is very beginner friendly** at the same time. 

For each paper, we will discuss the following questions:

1. Big Idea: What is the big idea for this paper? Or the main point the authors are trying to get across?
2. Relevance: Why does this research matter? I.e. Has this progressed anything else forward?
3. Open Questions: What open-ended questions do you have after reading this piece?

At the end of each discussion, we choose the next paper based on a vote. **Anyone** can suggest a paper and notes will be uploaded after each session.

We've started recording some sessions. You can see the recordings here 

- [A Comprehensive Overview of Large Language Models](https://www.youtube.com/watch?v=6zum9nOt7PM&t=43s)
- [Breaking down the OG GPT Paper by Alec Radford](https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=20s)

## Timeline

Currently here are the papers which we've covered or are covering

1. [Attention Is All You Need](/papers/attention-is-all-you-need): This was the initial paper which launched the LLM revolution with its newly introduced Attention Mechanism. It uses a decoder-encoder architecture in contract to the decoder only architectures today but is an interesting read.
2. Self Rewarding Language Models: How we can improve model performance on tasks by conditioning on their ability to evaluate and generate their outputs.
3. [BERT](/papers/bert) : BERT is a encoder-only model that aims to encode rich contextual understanding of a sentence. It was released in 2018 and was crucial in helping improve the performance of Google's search engine significantly.
4. [T5](/papers/text-to-text): T5 was a transformer model that was trained on the C4 corpus. The paper itself explores how we can exploit contextual representations learned by language models trained using an unsupervised approach for specific tasks with transfer learning. It also introduces the C4 dataset.
5. A Comprehensive Overview of Large Language Models : A guide to the major advancements and changes that have occured in the language modelling space
6. [GPT-2 ](/papers/gpt-2): Utilizing GPT-2, this paper matched serveral state of the art models across various benchmark without using custom architecture or dataset adjustments, solely through ample model capacity and data.
7. [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](/papers/deepseek-moe) : An introduction to the DeepSeek MOE model which beats LLama2-7B with only 40% of equivalent computation. 
8. Improving Language Understanding by Generative Pre-Training : This was the original GPT-1 paper by Alec Radford that showed how we can use generative pre-trained models to adapt to a variety of different tasks without having to create a task-specific architecture.
9. [Mixture Of Depths](/papers/mixture-of-depths): We train transformer models to be able to dynamically route tokens on a layer level so that we reduce the computation required for the attention computation
10. Mamba : An introduction to a new transformer alternative using State Space Models. See notes [here](https://blackbeelabs.notion.site/A-Mamba-Deep-Dive-4b9ceb34026e424982ca1342573cc43f?pvs=4)
11. [Medusa](/papers/medusa) : How to train new language modelling heads for speculative decoding and distil a dataset from the original model to prevent model drift while training the new model.