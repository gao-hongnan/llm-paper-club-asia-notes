# Introduction

These are our notes for the LLM Paper Club (Asia Edition!) run out of the `latent-space` discord channel. If you're interested in attending, you can check out the event calender [here](https://lu.ma/ls) of all the latent space event. 

## Simple Rules

In short, each week we discuss a LLM paper. We'd appreciate if you could read the paper beforehand so that when we work through it, it'd be a fruitful discussion for everyone. I'd like to emphasize here that **this is very beginner friendly** at the same time. 

For each paper, we will discuss the following questions:

1. Big Idea: What is the big idea for this paper? Or the main point the authors are trying to get across?
2. Relevance: Why does this research matter? I.e. Has this progressed anything else forward?
3. Open Questions: What open-ended questions do you have after reading this piece?

At the end of each discussion, we choose the next paper based on a vote. **Anyone** can suggest a paper and notes will be uploaded after each session. I'd like to also stress that the sessions will be **unrecorded** to protect the privacy of participants.

## Timeline

Currently here are the papers which we've covered or are covering

1. [Attention Is All You Need](/papers/attention-is-all-you-need): This was the initial paper which launched the LLM revolution with its newly introduced Attention Mechanism. It uses a decoder-encoder architecture in contract to the decoder only architectures today but is an interesting read.
2. Self Rewarding Language Models: How we can improve model performance on tasks by conditioning on their ability to evaluate and generate their outputs.
3. [BERT](/papers/bert.mdx) : BERT is a encoder-only model that aims to encode rich contextual understanding of a sentence. It was released in 2018 and was crucial in helping improve the performance of Google's search engine significantly.
4. [T5](/papers/text-to-text.mdx): T5 was a transformer model that was trained on the C4 corpus. The paper itself explores how we can exploit contextual representations learned by language models trained using an unsupervised approach for specific tasks with transfer learning. It also introduces the C4 dataset.