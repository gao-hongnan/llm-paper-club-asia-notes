# Introduction

These are our notes for the LLM Paper Club (Asia Edition!) run out of the `latent-space` discord channel. If you're interested in attending, you can check out the event calender [here](https://lu.ma/ls) of all the latent space event. 

## Simple Rules

In short, each week we discuss a LLM paper. We'd appreciate if you could read the paper beforehand so that when we work through it, it'd be a fruitful discussion for everyone. I'd like to emphasize here that **this is very beginner friendly** at the same time. 

For each paper, we will discuss the following questions:

1. Big Idea: What is the big idea for this paper? Or the main point the authors are trying to get across?
2. Relevance: Why does this research matter? I.e. Has this progressed anything else forward?
3. Open Questions: What open-ended questions do you have after reading this piece?

At the end of each discussion, we choose the next paper based on a vote. **Anyone** can suggest a paper and notes will be uploaded after each session. I'd like to also stress that the sessions will be **unrecorded** to protect the privacy of participants.

## Timeline

Currently here are the papers which we've covered or are covering

1. [Attention Is All You Need](/papers/attention-is-all-you-need): This was the initial paper which launched the LLM revolution with its newly introduced Attention Mechanism. It uses a decoder-encoder architecture in contract to the decoder only architectures today but is an interesting read.